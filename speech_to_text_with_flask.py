# -*- coding: utf-8 -*-
"""speech_to_text_with_flask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pz_xt1ugdfbZ_Mo3h4UTPGlQVuemrP1A
"""

!pip install flask pyngrok transformers torchaudio soundfile

!apt-get update && apt-get install ffmpeg -y

!mkdir -p templates
!mkdir -p static++
!mkdir -p uploads

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, render_template, request
# import functools
# import os
# import subprocess
# import torch
# import torchaudio
# 
# app = Flask(__name__)
# UPLOAD_FOLDER = "uploads"
# os.makedirs(UPLOAD_FOLDER, exist_ok=True)
# 
# # ‚úÖ Lazy-load Wav2Vec2 model instead of Whisper
# @functools.lru_cache(maxsize=1)
# def get_model():
#     from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
#     model_name = "facebook/wav2vec2-large-960h"
#     processor = Wav2Vec2Processor.from_pretrained(model_name)
#     model = Wav2Vec2ForCTC.from_pretrained(model_name)
#     model.eval()
#     return processor, model
# 
# @app.route("/", methods=["GET", "POST"])
# def home():
#     transcription = ""
#     if request.method == "POST":
#         if "audio_file" in request.files:
#             file = request.files["audio_file"]
#             if file.filename != "":
#                 original_path = os.path.join(UPLOAD_FOLDER, file.filename)
#                 file.save(original_path)
# 
#                 # üéß Convert audio to 16kHz mono WAV using ffmpeg
#                 wav_path = os.path.join(UPLOAD_FOLDER, "converted.wav")
#                 try:
#                     subprocess.run([
#                         "ffmpeg", "-y", "-i", original_path,
#                         "-ar", "16000", "-ac", "1", "-c:a", "pcm_s16le",
#                         wav_path
#                     ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
# 
#                     # üîä Load audio with torchaudio
#                     speech_array, _ = torchaudio.load(wav_path)
# 
#                     # üß† Load Wav2Vec2 model and processor
#                     processor, model = get_model()
# 
#                     # üß© Prepare input tensors
#                     input_values = processor(
#                         speech_array[0],
#                         sampling_rate=16000,
#                         return_tensors="pt"
#                     ).input_values
# 
#                     # üöÄ Run inference
#                     with torch.no_grad():
#                         logits = model(input_values).logits
# 
#                     predicted_ids = torch.argmax(logits, dim=-1)
#                     transcription = processor.decode(predicted_ids[0])
# 
#                 except Exception as e:
#                     transcription = f"‚ùå Error during transcription: {e}"
# 
#     return render_template("index.html", transcription=transcription)
# 
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=8000, debug=False)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile templates/index.html
# <!DOCTYPE html>
# <html>
# <head>
#     <title>üéôÔ∏è Speech-to-Text Transcription</title>
#     <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
# </head>
# <body>
#     <div class="container">
#         <h1>üéôÔ∏è Speech-to-Text Transcription</h1>
# 
#         <div class="card">
#             <h2>Record Your Voice</h2>
#             <button id="recordBtn">Start Recording</button>
#             <button id="stopBtn" disabled>Stop Recording</button>
#             <audio id="audioPlayback" controls></audio>
#             <a id="downloadLink" href="#" download="recorded_audio.webm">Download Audio</a>
#         </div>
# 
#         <div class="card">
#             <h2>Upload Audio File</h2>
#             <form method="post" enctype="multipart/form-data">
#                 <input type="file" name="audio_file" accept="audio/*" required>
#                 <button type="submit">Transcribe üöÄ</button>
#             </form>
#         </div>
# 
#         {% if transcription %}
#         <div class="result">
#             <h2>‚úÖ Transcription:</h2>
#             <p>{{ transcription }}</p>
#         </div>
#         {% endif %}
#     </div>
# 
# <script>
# let mediaRecorder;
# let audioChunks = [];
# 
# const recordBtn = document.getElementById("recordBtn");
# const stopBtn = document.getElementById("stopBtn");
# const audioPlayback = document.getElementById("audioPlayback");
# const downloadLink = document.getElementById("downloadLink");
# 
# recordBtn.onclick = async () => {
#     let stream = await navigator.mediaDevices.getUserMedia({ audio: true });
#     mediaRecorder = new MediaRecorder(stream); // default MIME (WebM)
#     mediaRecorder.start();
#     audioChunks = [];
# 
#     mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
#     mediaRecorder.onstop = e => {
#         const blob = new Blob(audioChunks, { type: 'audio/webm' });
#         audioPlayback.src = URL.createObjectURL(blob);
#         downloadLink.href = audioPlayback.src;
#     };
# 
#     recordBtn.disabled = true;
#     stopBtn.disabled = false;
# };
# 
# stopBtn.onclick = () => {
#     mediaRecorder.stop();
#     recordBtn.disabled = false;
#     stopBtn.disabled = true;
# };
# </script>
# </body>
# </html>
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile static++/style.css
# body {
#     font-family: 'Segoe UI', sans-serif;
#     background: linear-gradient(135deg, #141E30, #243B55);
#     color: white;
#     display: flex;
#     justify-content: center;
#     align-items: center;
#     height: 100vh;
#     margin: 0;
# }
# .container {
#     text-align: center;
#     width: 60%;
# }
# h1, h2 {
#     margin-bottom: 15px;
# }
# .card {
#     background: rgba(255, 255, 255, 0.1);
#     padding: 20px;
#     border-radius: 12px;
#     box-shadow: 0 0 15px rgba(0,0,0,0.4);
#     margin-bottom: 20px;
# }
# input, button, audio, a {
#     width: 90%;
#     margin: 10px 0;
#     padding: 10px;
#     border-radius: 8px;
#     border: none;
# }
# button {
#     background: #FFD700;
#     cursor: pointer;
#     font-weight: bold;
# }
# button:hover {
#     background: #FFA500;
# }
# .result {
#     margin-top: 20px;
#     padding: 15px;
#     background: rgba(255,255,255,0.1);
#     border-radius: 10px;
# }
#

# Kill any previous processes
!pkill -f flask || echo "No flask running"
!pkill -f ngrok || echo "No ngrok running"

!lsof -i :8000

!kill -9 1374 # here ID no shud be changed based on the above cell { PID 1949 }

# Run Flask in the background
!nohup python app.py > flask.log 2>&1 &

# Start ngrok tunnel
from pyngrok import ngrok, conf
conf.get_default().auth_token = "33pyU8XWfrN8JFX8kwzY8Uf1j7s_612rotnaQX2f58dq3n8Ez"

public_url = ngrok.connect(8000)
print("üåç Public URL:", public_url)

# Optional: check logs
!sleep 3 && tail -n 20 flask.log

